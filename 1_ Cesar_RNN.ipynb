{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c96c4c3-aaa6-40fe-9490-f6f254e80242",
   "metadata": {},
   "source": [
    "## Алгоритм шифра Цезаря"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ab2b0a-3a77-4740-bcce-04d42fd23c78",
   "metadata": {},
   "source": [
    "Мы будем использовать английский алфавит, который состоит из 26 букв. Сдвиг буквы осуществляется по модулю 26. Вот код на Python для шифрования и дешифрования, а также для генерации выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7af27d63-3af7-4ab3-a3fa-4f64551300ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Определяем алфавиты\n",
    "ALPHABETS = {\n",
    "    'russian': 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя ',\n",
    "    'english': 'abcdefghijklmnopqrstuvwxyz ',\n",
    "    'german': 'abcdefghijklmnopqrstuvwxyzäöüß '\n",
    "}\n",
    "\n",
    "# Создание соответствующих словарей\n",
    "CHAR_TO_INDEX = {lang: {char: idx for idx, char in enumerate(alphabet)} for lang, alphabet in ALPHABETS.items()}\n",
    "INDEX_TO_CHAR = {lang: {idx: char for idx, char in enumerate(alphabet)} for lang, alphabet in ALPHABETS.items()}\n",
    "\n",
    "# Определяем размеры входных данных и выходов\n",
    "INPUT_SIZES = {lang: len(alphabet) for lang, alphabet in ALPHABETS.items()}\n",
    "HIDDEN_SIZE = 128  # Определите размер скрытого слоя\n",
    "OUTPUT_SIZES = {lang: len(alphabet) for lang, alphabet in ALPHABETS.items()}\n",
    "\n",
    "# Алгоритм шифра Цезаря\n",
    "def caesar_cipher(text, shift, alphabet):\n",
    "    result = \"\"\n",
    "    for char in text:\n",
    "        if char in alphabet:\n",
    "            # Находим индекс текущего символа\n",
    "            index = alphabet.index(char)\n",
    "            # Применяем сдвиг\n",
    "            new_index = (index + shift) % len(alphabet)  # Зацикливание по алфавиту\n",
    "            result += alphabet[new_index]\n",
    "        else:\n",
    "            result += char  # Не изменяем, если символ не в алфавите\n",
    "    return result\n",
    "\n",
    "# Генерация данных\n",
    "def generate_data(num_samples=1000, shift=2, language ='russian'):\n",
    "    alphabet = ALPHABETS[language]\n",
    "    samples = []\n",
    "    for _ in range(num_samples):\n",
    "        length = random.randint(5, 15)  # Случайная длина строки\n",
    "        random_text = ''.join(random.choices(alphabet, k=length))  # Генерация случайного текста\n",
    "        encoded_text = caesar_cipher(random_text, shift, alphabet)  # Шифрование текста\n",
    "        samples.append((encoded_text, random_text))  # (защифрованный текст, оригинальный текст)\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928906c1-5338-4ab5-8621-0c27725e6a63",
   "metadata": {},
   "source": [
    "## Подготовка данных для обучения RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2ed1d8d-f725-4e4a-b8a4-6b99e6fa5184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Подготовка данных для обучения RNN\n",
    "class CipherDataset(Dataset):\n",
    "    def __init__(self, data, language):\n",
    "        if language not in ALPHABETS:\n",
    "            raise ValueError(\"Unsupported language. Please choose 'russian', 'english', or 'german'.\")\n",
    "        \n",
    "        self.data = data\n",
    "        self.language = language\n",
    "        self.char_to_index = CHAR_TO_INDEX[self.language]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded, original = self.data[index]\n",
    "        # Преобразуем каждый символ в индекс\n",
    "        return (\n",
    "            torch.tensor([self.char_to_index[char] for char in encoded], dtype=torch.long),\n",
    "            torch.tensor([self.char_to_index[char] for char in original], dtype=torch.long)\n",
    "        )\n",
    "\n",
    "# Функция для добавления паддинга к последовательностям\n",
    "def collate_fn(batch):\n",
    "    encoded_inputs = [item[0] for item in batch]\n",
    "    original_outputs = [item[1] for item in batch]\n",
    "    \n",
    "    max_length = max(len(seq) for seq in encoded_inputs)\n",
    "    \n",
    "    padded_encoded = torch.stack([\n",
    "        torch.cat([seq, torch.zeros(max_length - len(seq), dtype=torch.long)]) for seq in encoded_inputs\n",
    "    ])\n",
    "    \n",
    "    padded_original = torch.stack([\n",
    "        torch.cat([seq, torch.zeros(max_length - len(seq), dtype=torch.long)]) for seq in original_outputs\n",
    "    ])\n",
    "    \n",
    "    return padded_encoded, padded_original\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d735ea6-0fe9-4bed-9710-687ddd559282",
   "metadata": {},
   "source": [
    "## Создание архитектуры рекуррентной нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fa02ca7-23df-4161-a2ec-1b1011a71658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание архитектуры RNN\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, 30)\n",
    "        self.rnn = nn.RNN(input_size=30, hidden_size=HIDDEN_SIZE, batch_first=True)\n",
    "        self.fc = nn.Linear(HIDDEN_SIZE, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        rnn_out, _ = self.rnn(x)  # Выходы для всех временных шагов\n",
    "        return self.fc(rnn_out)  # Вернуть все выходы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164a943e-f7a5-4aa9-8650-c06c994d4fd3",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f875b77-f487-48d6-b98b-3122f4a40311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели\n",
    "def train(model, dataloader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for encoded_x, original_y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(encoded_x)  # Получаем выход от модели\n",
    "\n",
    "            output = output.view(-1, OUTPUT_SIZES[dataloader.dataset.language])  # (batch_size * seq_len, OUTPUT_SIZE)\n",
    "            original_y = original_y.view(-1)  # (batch_size * seq_len)\n",
    "            loss = criterion(output, original_y)  # Рассчитываем потери\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90da3661-a2f1-44f7-8f91-e29f6cdf32bd",
   "metadata": {},
   "source": [
    "## Проверка качества модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15c0fc7f-86de-46bf-8f4d-e1d37af88e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка качества модели на зашифрованном входе\n",
    "def evaluate(model, encoded_text, language):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor([CHAR_TO_INDEX[language][char] for char in encoded_text]).view(1, -1)\n",
    "        output = model(input_tensor)\n",
    "        output = output.view(-1, OUTPUT_SIZES[language])  # Изменяем размерность\n",
    "        _, predicted_indices = torch.max(output, -1)\n",
    "        decoded_text = ''.join([INDEX_TO_CHAR[language][idx.item()] for idx in predicted_indices])\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b054ef-5641-4d41-aac8-6ba524aaf460",
   "metadata": {},
   "source": [
    "## Запуск процесса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6defdb94-d15e-4a24-8d23-8d15cd0a3d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 2.222014993429184\n",
      "Epoch 2/20, Loss: 1.0577965434640646\n",
      "Epoch 3/20, Loss: 0.38165406975895166\n",
      "Epoch 4/20, Loss: 0.15720793581567705\n",
      "Epoch 5/20, Loss: 0.10248364065773785\n",
      "Epoch 6/20, Loss: 0.07826559769455343\n",
      "Epoch 7/20, Loss: 0.06509139016270638\n",
      "Epoch 8/20, Loss: 0.057547241100110114\n",
      "Epoch 9/20, Loss: 0.052253272384405136\n",
      "Epoch 10/20, Loss: 0.047270149108953774\n",
      "Epoch 11/20, Loss: 0.04495182994287461\n",
      "Epoch 12/20, Loss: 0.04201121750520542\n",
      "Epoch 13/20, Loss: 0.03941555641358718\n",
      "Epoch 14/20, Loss: 0.03635356843005866\n",
      "Epoch 15/20, Loss: 0.035434294084552675\n",
      "Epoch 16/20, Loss: 0.03414078365312889\n",
      "Epoch 17/20, Loss: 0.030787639145273715\n",
      "Epoch 18/20, Loss: 0.029676320729777217\n",
      "Epoch 19/20, Loss: 0.02688651467906311\n",
      "Epoch 20/20, Loss: 0.02438505698228255\n",
      "Encoded:  qahlpld\n",
      "Original: yozfjnjb\n",
      "Decoded: yozfjnjb\n",
      "\n",
      "Encoded: uxefndup\n",
      "Original: svcdlbsn\n",
      "Decoded: svcdlbsn\n",
      "\n",
      "Encoded: blhipubuuwfgoot\n",
      "Original:  jfgns ssudemmr\n",
      "Decoded:  jfgns ssudemmr\n",
      "\n",
      "Encoded: qfztan\n",
      "Original: odxrzl\n",
      "Decoded: odxrzl\n",
      "\n",
      "Encoded: lvhcsloleh\n",
      "Original: jtfaqjmjcf\n",
      "Decoded: jtfaqjmjcf\n",
      "\n",
      "Encoded: iqgjn\n",
      "Original: goehl\n",
      "Decoded: goehl\n",
      "\n",
      "Encoded: qpbled\n",
      "Original: on jcb\n",
      "Decoded: on jcb\n",
      "\n",
      "Encoded: ho rhojz\n",
      "Original: fmypfmhx\n",
      "Decoded: fmypfmhx\n",
      "\n",
      "Encoded: pksvgzesafpyzn \n",
      "Original: niqtexcqzdnwxly\n",
      "Decoded: niqtexcqadnwxly\n",
      "\n",
      "Encoded: unoce\n",
      "Original: slmac\n",
      "Decoded: slmac\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Задаем параметры\n",
    "    language = 'english'  # можно русский, английский, немецкий \n",
    "    num_samples = 1000\n",
    "    shift = 2\n",
    "\n",
    "    # Генерация данных\n",
    "    data = generate_data(num_samples=num_samples, shift=shift, language=language)\n",
    "\n",
    "    # Подготовка и распределение данных\n",
    "    dataset = CipherDataset(data, language=language)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "   # Инициализация модели\n",
    "    model = SimpleRNN(INPUT_SIZES[language])  # INPUT_SIZE определяет размер входа по алфавиту\n",
    "\n",
    "   # Определение функции потерь\n",
    "    criterion = nn.CrossEntropyLoss()  # Для задач многоклассовой классификации\n",
    "\n",
    "   # Инициализация оптимизатора\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)  # Устанавливаем Adam как оптимизатор с начальной скоростью обучения 0.001\n",
    "   # Обучение модели\n",
    "    train(model, dataloader, criterion, optimizer, num_epochs=20)\n",
    "\n",
    "    # Пример для проверки\n",
    "    test_samples = generate_data(10, shift=shift, language=language)\n",
    "    for encoded, original in test_samples:\n",
    "        decoded = evaluate(model, encoded, language)\n",
    "        print(f'Encoded: {encoded}\\nOriginal: {original}\\nDecoded: {decoded}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d140181-f8ed-4499-86ac-d030427c394e",
   "metadata": {},
   "source": [
    "### Выводы: \n",
    "1.Снижение потерь сигнализирует о том, что модель обучается и в состоянии улучшать свои предсказания. В идеале, значение потерь стремвится к 0, однако наличие некоторого остаточного значения в 0.024 говорит о том, что модель все равно может иметь место для улучшения.\n",
    "2.Примерная оценка точности составляет 80%, что является хорошим результатом для такой задачи.\n",
    "3. В большинстве случаев модель точно расшифровывает входные данные, однако есть несколько незначительных ошибок, которые могут быть улучшены за счет увеличения объемов данных, использования GRU для улучшения обработки последовательностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee4436-9507-4b1c-96a8-fe5070e645f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
